#!/usr/bin/env python3

import argparse
import socket
import ssl
import re
from html.parser import HTMLParser

DEFAULT_SERVER = "proj5.3700.network"
DEFAULT_PORT = 443

datalist = []
taglist = []
endtaglist = []

class MyHTMLParser(HTMLParser):

    def handle_data(self, data):
        datalist.append(data)

    def handlestart_tag(self, tag, attrs):
        for attr in attrs:
            taglist.append(attr)

    def handleend_tag(self, tag):
        endtaglist.append(tag)


class Crawler:
    def __init__(self, args):
        self.server = args.server
        self.port = args.port
        self.username = args.username
        self.password = args.password
        self.csrf = None
        self.sessionid = None
        self.middleware = ""
        self.testlist = []
        self.parser = MyHTMLParser()
        self.visited = []
        self.queue = []


    def run(self):
        #request = "GET / HTTP/1.0\r\n\r\n"

        print("Request to %s:%d" % (self.server, self.port))
        self.loginProcess()
        

    def findcookievariables(self, edata):
        datalist = edata.split()
        for i in datalist:
            if "csrftoken" in i:
                self.csrf= i[:len(i) -1]
            elif "sessionid" in i:
                self.sessionid = i[:len(i) - 1]
            elif "value" in i:
                self.testlist.append(i)

        #crafts the middleware token, to be the specific index
        self.middleware = self.testlist[0]
        self.middleware = self.middleware[:len(self.middleware) - 2]
        self.middleware = self.middleware[7 : len(self.middleware)]

    #rebuild the socket with each request
    def BuildGetRequest(self, url, cookies):
        request = "GET " + url + " HTTP/1.0\r\n"
        if(cookies):
            cookiestring = "Cookie: " + self.csrf + "; " + self.sessionid + "\r\n"
            request = request + cookiestring

        request = request + "\r\n"
        return request     

    def BuildNewSocket(self, httpR):
        mysocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        mysocket = ssl.wrap_socket(mysocket)
        mysocket.connect((self.server, self.port))
        mysocket.send(httpR.encode('ascii'))
        data = mysocket.recv(90000).decode('ascii')
        #need to fix this recieve method since it can sometimes drop headers...
        print("Response: "+ data)
        mysocket.close()
        return data


    def buildloginpostr(self):
        payload  = "username=" + self.username + "&password=" + self.password + "&csrfmiddlewaretoken="+ self.middleware + "&next=%2Ffakebook%2F\r\n\r\n"
        Contentlength = len(payload)
        prequest = "POST /accounts/login/ HTTP/1.0\r\n" + "Cookie: "+ self.csrf + "; " + self.sessionid + "\r\n" + "Content-Type: application/x-www-form-urlencoded\r\n" + "Connection: close\r\n" + "Content-length: " + str(Contentlength) + "\r\n" + "Referrer: https://proj5.3700.network/accounts/login/?next=/fakebook/\r\n\r\n"  
        post = prequest + payload
        return post

    def loginProcess(self):
        #connects to the intial website.
        intialconnect = "/"
        req = self.BuildGetRequest(intialconnect, False)
        msg = self.BuildNewSocket(req)

        #gets to the log in 
        nextpage = "/accounts/login/?next=/fakebook/"
        req = self.BuildGetRequest(nextpage, False)
        msg = self.BuildNewSocket(req)
        self.findcookievariables(msg)

        #builds the log in post rquest
        loginpost = self.buildloginpostr()
        msg = self.BuildNewSocket(loginpost)
        self.findcookievariables(msg)

        #continues the get request to the home page
        homepage = "/fakebook/"
        req = self.BuildGetRequest(homepage, True)
        msg = self.BuildNewSocket(req)

        #return the last message? so that the crawler knows where to begin




        





if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='crawl Fakebook')
    parser.add_argument('-s', dest="server", type=str, default=DEFAULT_SERVER, help="The server to crawl")
    parser.add_argument('-p', dest="port", type=int, default=DEFAULT_PORT, help="The port to use")
    parser.add_argument('username', type=str, help="The username to use")
    parser.add_argument('password', type=str, help="The password to use")
    args = parser.parse_args()
    sender = Crawler(args)
    sender.run()
